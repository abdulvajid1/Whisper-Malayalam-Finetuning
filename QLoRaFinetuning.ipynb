{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "o28PTZGUfTyS",
      "metadata": {
        "id": "o28PTZGUfTyS"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade datasets -q\n",
        "# !pip install jiwer -q\n",
        "# !pip install evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "04cc562d",
      "metadata": {
        "id": "04cc562d"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "# from torchaudio import load\n",
        "import evaluate\n",
        "\n",
        "\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from transformers import BitsAndBytesConfig\n",
        "# Load Whisper processor\n",
        "from transformers import WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer\n",
        "from transformers import WhisperForConditionalGeneration\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54e301e8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ffea934c",
      "metadata": {
        "id": "ffea934c"
      },
      "outputs": [],
      "source": [
        "\n",
        "load_dotenv()\n",
        "login_token = os.getenv('HuggingFaceToken')\n",
        "\n",
        "login(login_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce19b42",
      "metadata": {
        "id": "5ce19b42"
      },
      "source": [
        "## Data Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "31731923",
      "metadata": {
        "id": "31731923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
            "        num_rows: 2023\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
            "        num_rows: 710\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\n",
        "    \"mozilla-foundation/common_voice_17_0\", \"ml\", split=\"train+validation\",\n",
        ")\n",
        "common_voice[\"test\"] = load_dataset(\n",
        "    \"mozilla-foundation/common_voice_17_0\", \"ml\", split=\"test\",\n",
        ")\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "788cad4a",
      "metadata": {
        "id": "788cad4a"
      },
      "outputs": [],
      "source": [
        "# remove unwanted features\n",
        "common_voice = common_voice.select_columns(['audio', 'sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "71afaaba",
      "metadata": {
        "id": "71afaaba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'path': 'C:\\\\Users\\\\VICTUS\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\3e7b12b0fa0deddeccc4a37a644801109d30fe7dda8b39a953688d0be0744a2f\\\\ml_train_0/common_voice_ml_37003897.mp3', 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "       1.33694380e-06, 6.72575652e-07, 1.44025307e-07], shape=(150336,)), 'sampling_rate': 48000}\n"
          ]
        }
      ],
      "source": [
        "print(common_voice['train'][0]['audio'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6d5b848d",
      "metadata": {
        "id": "6d5b848d"
      },
      "outputs": [],
      "source": [
        "# 48kHz -> 16kHz\n",
        "from datasets import Audio\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f6c6b187",
      "metadata": {
        "id": "f6c6b187"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "       -1.61271237e-06, -1.26397367e-06,  1.32478658e-06], shape=(50112,))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "common_voice['train'][0]['audio']['array']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9f35b2e7",
      "metadata": {
        "id": "9f35b2e7"
      },
      "outputs": [],
      "source": [
        "# def collate_fuc(batch):\n",
        "#     print(len(batch))\n",
        "#     print(batch[0].keys())\n",
        "# data_loader = DataLoader(common_voice['train'], batch_size=3, shuffle=True, collate_fn=collate_fuc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1d45832d",
      "metadata": {
        "id": "1d45832d"
      },
      "outputs": [],
      "source": [
        "# for i in data_loader:\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b55252f5",
      "metadata": {
        "id": "b55252f5"
      },
      "outputs": [],
      "source": [
        "# filtering audio len > 30 sec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a75ce08e",
      "metadata": {
        "id": "a75ce08e"
      },
      "outputs": [],
      "source": [
        "# whisper proccessor wrap whisperFeature extractor for audio and whispertokenizer for text labels as one processor\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained('openai/whisper-small', task='transcribe', language='malayalam')\n",
        "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-small', task='transcribe', language='malayalam')\n",
        "processor = WhisperProcessor.from_pretrained('openai/whisper-small', task='transcribe', language='malayalam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6eea5ca5",
      "metadata": {
        "id": "6eea5ca5"
      },
      "outputs": [],
      "source": [
        "# sample_tokens = tokenizer.encode('ഇല്ല മോനേ')\n",
        "# tokenizer.decode(sample_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "db6558f9",
      "metadata": {
        "id": "db6558f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|startoftranscript|><|ml|><|transcribe|><|notimestamps|>ഇല്ല മോനേ<|endoftext|>'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'ഇല്ല മോനേ'\n",
        "batch = processor(text=text, sampling_rate=16000)\n",
        "processor.tokenizer.decode(batch['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7a621160",
      "metadata": {
        "id": "7a621160"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "def prepare_data(batch):\n",
        "    # processor have both feature extractor for audio and tokenizer for text, so we just pass both of theem\n",
        "    batch = processor(audio=batch['audio']['array'],\n",
        "                      text=batch['sentence'],\n",
        "                      sampling_rate=processor.feature_extractor.sampling_rate)\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "_smrP8WQclQ8",
      "metadata": {
        "id": "_smrP8WQclQ8"
      },
      "outputs": [],
      "source": [
        "MAX_LABEL_TOKEN_WHISPER_SUPPORT = 448\n",
        "def filter_label_token(batch):\n",
        "    return len(batch['labels']) <= MAX_LABEL_TOKEN_WHISPER_SUPPORT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d9ce986",
      "metadata": {
        "id": "6d9ce986"
      },
      "outputs": [],
      "source": [
        "common_voice = common_voice.map(prepare_data, batched=False)\n",
        "common_voice = common_voice.filter(filter_label_token, batched=False)\n",
        "common_voice = common_voice.select_columns(['input_features', 'labels'])\n",
        "common_voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7f734de1",
      "metadata": {
        "id": "7f734de1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 80, 3000])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.tensor(common_voice['train'][0]['input_features']).shape # (1, 80, 3000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291e40c5",
      "metadata": {
        "id": "291e40c5"
      },
      "source": [
        "Dataloader takes random datapoints, here it will look like {input_feature:.., labels}, when batch enabled it will be like [{inp:.., lable:..}, {inp: .., label:..}], we need to use data collator for pad them and join them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3f6badd5",
      "metadata": {
        "id": "3f6badd5"
      },
      "outputs": [],
      "source": [
        "# feature_extractor.pad(common_voice['train'][:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "94f20449",
      "metadata": {
        "id": "94f20449"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(common_voice['train'][0]['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b47615af",
      "metadata": {
        "id": "b47615af"
      },
      "outputs": [],
      "source": [
        "# Data Collator for padding\n",
        "\n",
        "class DataCollatorForSeqToSeqPadding:\n",
        "    def __init__(self, processor: WhisperProcessor):\n",
        "        self.processor = processor\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # batch = [ {'input_feature':[], labels:[]}, {} ...]\n",
        "        input_features = [{\"input_features\" : data['input_features'][0]} for data in batch]\n",
        "        labels = [{\"input_ids\" : data['labels']} for data in batch]\n",
        "\n",
        "        # feature extractor from hugging face already support padding to {'input_features':[]}\n",
        "        # padding using feature extractor for audio and tokenizer for labels\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors='pt') # created a batch object , later will add label to this too, that's how huggingface model expect data {'input_features':[], labels:[]}\n",
        "\n",
        "        # whisper tokenizer.pad will check the {'input_ids':[]} for padding and return in same forma\n",
        "        labels = self.processor.tokenizer.pad(labels, return_tensors='pt')\n",
        "\n",
        "\n",
        "        # since we are using hugging face model we don't need to stack the tensor cuz the hugging face (whisper here) model expect input like {'input_features':[], labels:[]}\n",
        "        # tensor stacking\n",
        "        # input_features = torch.stack(input_features, dim=0)\n",
        "        # labels = torch.stack(labels) # have\n",
        "\n",
        "\n",
        "        labels = labels['input_ids'].masked_fill(labels['attention_mask'].eq(0), -100)\n",
        "\n",
        "        # we are removing the start token since the hugging face model design to automatically add start token\n",
        "        # by doing shifting labels to right [1, 2, <\\s>] -> [<s>, 1, 2], where we using this shifted tensor as input\n",
        "        # and the non shifted as the labels to calculate the loss (the model gets what's his start token from the config)\n",
        "\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            print()\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "\n",
        "        return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c849455e",
      "metadata": {
        "id": "c849455e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|endoftext|>'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(processor.tokenizer.bos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "380221a8",
      "metadata": {
        "id": "380221a8"
      },
      "outputs": [],
      "source": [
        "collate_fn = DataCollatorForSeqToSeqPadding(processor=processor)\n",
        "\n",
        "# data loader for just checking the data collator, seqtoseq trainer does not need dataloader (inbuilt)\n",
        "# data_loader = DataLoader(dataset=common_voice['train'],\n",
        "#                          collate_fn=collate_fn,\n",
        "#                          batch_size=2,\n",
        "#                          shuffle=True,\n",
        "#                          drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa93162",
      "metadata": {
        "id": "daa93162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_features', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "# for batch in data_loader:\n",
        "#     print(batch.keys()) # torch.Size([2, 80, 3000]), Yes now it's coming as batch size and not in (2, 1, 80, 3000)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf94029b",
      "metadata": {
        "id": "bf94029b"
      },
      "source": [
        "## Model setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "bd32e5a6",
      "metadata": {
        "id": "bd32e5a6"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load('wer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5Fzcwve8jB7s",
      "metadata": {
        "id": "5Fzcwve8jB7s"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor.tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "316f4a6b",
      "metadata": {
        "id": "316f4a6b"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    # pred will look like {'label_ids':[torch.tensor], prediction:[torch.tensor]}\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # change all -100 value which we set for loss calculation back to padding since we are calculating wer\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # convert to string and remove the padding token if there is.. if it was -100 then it won't work that is why we changed back to padding\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Now compute, the metric comes from the evaluate and we set it in the arugment of Trainer so compute metric use this metric here\n",
        "    wer = metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {'wer': wer} # standard form of hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5eba7826",
      "metadata": {},
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, \"float16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f85a4add",
      "metadata": {},
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f80821",
      "metadata": {
        "id": "94f80821"
      },
      "source": [
        "### Lora Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "798841f5",
      "metadata": {
        "id": "798841f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'xpu', 'npu', 'mps', 'cuda', 'hpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'xpu', 'npu', 'mps', 'cuda', 'hpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Choose model size here\u001b[39;00m\n\u001b[32m      2\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mopenai/whisper-small\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mWhisperForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# forced decoder ids automatically add tokens at specified position (1, tokenizer.bos_token), so at decoder time the model automaticall generate it\u001b[39;00m\n\u001b[32m      6\u001b[39m model.config.forced_decoder_ids = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4390\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4387\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4390\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4396\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4397\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4398\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:84\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n\u001b[32m     83\u001b[39m bnb_multibackend_is_enabled = is_bitsandbytes_multi_backend_available()\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mvalidate_bnb_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mfrom_tf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mfrom_flax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m sure the weights are in PyTorch format.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:560\u001b[39m, in \u001b[36mvalidate_bnb_backend_availability\u001b[39m\u001b[34m(raise_exception)\u001b[39m\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_bnb_multi_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _validate_bnb_cuda_backend_availability(raise_exception)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:517\u001b[39m, in \u001b[36m_validate_bnb_multi_backend_availability\u001b[39m\u001b[34m(raise_exception)\u001b[39m\n\u001b[32m    511\u001b[39m     err_msg = (\n\u001b[32m    512\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of the available devices `available_devices = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_devices\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbnb_supported_devices_with_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m     )\n\u001b[32m    516\u001b[39m     logger.error(err_msg)\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[32m    519\u001b[39m logger.warning(\u001b[33m\"\u001b[39m\u001b[33mNo supported devices found for bitsandbytes multi-backend.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[31mRuntimeError\u001b[39m: None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'xpu', 'npu', 'mps', 'cuda', 'hpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
          ]
        }
      ],
      "source": [
        "# Choose model size here\n",
        "model_id = \"openai/whisper-small\"\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_id, quantization_config=bnb_config, device_map='auto')\n",
        "\n",
        "# forced decoder ids automatically add tokens at specified position (1, tokenizer.bos_token), so at decoder time the model automaticall generate it\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460d1e91",
      "metadata": {
        "id": "460d1e91"
      },
      "outputs": [],
      "source": [
        "lora_r = 256\n",
        "lora_alpha = 512\n",
        "learning_rate = 5e-5\n",
        "peft_config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    # lora_dropout=0.0005,\n",
        "    bias='none',\n",
        "    target_modules=['q_proj', 'v_proj', 'out_proj', 'fc1', 'fc2'],\n",
        "    task_type=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "44V2F7zpVpCW",
      "metadata": {
        "id": "44V2F7zpVpCW"
      },
      "outputs": [],
      "source": [
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "08CX2DRqepbc",
      "metadata": {
        "id": "08CX2DRqepbc"
      },
      "outputs": [],
      "source": [
        "model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "24771f94",
      "metadata": {
        "id": "24771f94"
      },
      "outputs": [],
      "source": [
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "680728b2",
      "metadata": {
        "id": "680728b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 51,904,512 || all params: 293,639,424 || trainable%: 17.6763\n"
          ]
        }
      ],
      "source": [
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "8ec0c2af",
      "metadata": {
        "id": "8ec0c2af"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "1bd6dc1a",
      "metadata": {
        "id": "1bd6dc1a"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(output_dir='checkpoints',\n",
        "                                         eval_strategy='steps',\n",
        "                                         learning_rate=learning_rate,\n",
        "                                         gradient_checkpointing=True,\n",
        "                                         per_device_train_batch_size=4,\n",
        "                                         gradient_accumulation_steps=2,\n",
        "                                         warmup_steps=20,\n",
        "                                         predict_with_generate=True,\n",
        "                                         generation_max_length=35,\n",
        "                                         per_device_eval_batch_size=1,\n",
        "                                        #  eval_accumulation_steps=2,\n",
        "                                         fp16=True,\n",
        "                                         save_steps=100,\n",
        "                                         eval_steps=10,\n",
        "                                         logging_dir=f'runs/{lora_r}_{lora_alpha}_{learning_rate}',\n",
        "                                         report_to=['tensorboard'],\n",
        "                                         load_best_model_at_end=True,\n",
        "                                         metric_for_best_model='wer',\n",
        "                                         num_train_epochs=5,\n",
        "                                         torch_empty_cache_steps=5,\n",
        "                                         dataloader_drop_last=True,\n",
        "                                        #  dataloader_num_workers=2,\n",
        "                                        #  dataloader_pin_memory=True,\n",
        "                                         logging_strategy='steps',\n",
        "                                         logging_steps=10,\n",
        "                                         optim='adamw_torch',\n",
        "                                         label_names=['labels'],\n",
        "                                         lr_scheduler_type=\"cosine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "PITKfuOjquki",
      "metadata": {
        "id": "PITKfuOjquki"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric.compute(predictions=['hello', 'world'], references=['hello', 'there'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "76552312",
      "metadata": {
        "id": "76552312"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    train_dataset=common_voice['train'],\n",
        "    eval_dataset=common_voice['test'].shuffle(seed=0).select(range(20)),\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForSeqToSeqPadding(processor=processor),\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "2d16436d",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "0cd5ed5f",
      "metadata": {
        "id": "0cd5ed5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1265' max='1265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1265/1265 1:28:55, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.423700</td>\n",
              "      <td>2.429608</td>\n",
              "      <td>1.132530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.861700</td>\n",
              "      <td>1.832908</td>\n",
              "      <td>1.036145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.504600</td>\n",
              "      <td>1.472699</td>\n",
              "      <td>1.024096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.354500</td>\n",
              "      <td>1.310598</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.184000</td>\n",
              "      <td>1.141106</td>\n",
              "      <td>1.012048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.042100</td>\n",
              "      <td>1.004097</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.885100</td>\n",
              "      <td>0.837354</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.702072</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.677300</td>\n",
              "      <td>0.570596</td>\n",
              "      <td>0.963855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.587800</td>\n",
              "      <td>0.502280</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>0.467182</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.516400</td>\n",
              "      <td>0.420565</td>\n",
              "      <td>0.951807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.450400</td>\n",
              "      <td>0.409772</td>\n",
              "      <td>0.951807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.421800</td>\n",
              "      <td>0.358515</td>\n",
              "      <td>0.951807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.407200</td>\n",
              "      <td>0.363407</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.415600</td>\n",
              "      <td>0.330717</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.365100</td>\n",
              "      <td>0.328273</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.361100</td>\n",
              "      <td>0.298858</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.348800</td>\n",
              "      <td>0.323043</td>\n",
              "      <td>0.951807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.351200</td>\n",
              "      <td>0.318576</td>\n",
              "      <td>0.963855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.332700</td>\n",
              "      <td>0.285228</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.308500</td>\n",
              "      <td>0.292406</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.327400</td>\n",
              "      <td>0.283222</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.294700</td>\n",
              "      <td>0.289846</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.295600</td>\n",
              "      <td>0.270032</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.266000</td>\n",
              "      <td>0.260212</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.238900</td>\n",
              "      <td>0.265481</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.251100</td>\n",
              "      <td>0.252863</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.225100</td>\n",
              "      <td>0.248201</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.246800</td>\n",
              "      <td>0.242334</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.252400</td>\n",
              "      <td>0.229039</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.207400</td>\n",
              "      <td>0.213794</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.209600</td>\n",
              "      <td>0.235947</td>\n",
              "      <td>0.951807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.235900</td>\n",
              "      <td>0.232992</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.210100</td>\n",
              "      <td>0.244414</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.254600</td>\n",
              "      <td>0.223393</td>\n",
              "      <td>0.879518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.222800</td>\n",
              "      <td>0.228831</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.215100</td>\n",
              "      <td>0.212406</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.230200</td>\n",
              "      <td>0.217645</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.212100</td>\n",
              "      <td>0.212074</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.227900</td>\n",
              "      <td>0.220321</td>\n",
              "      <td>0.951807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.209800</td>\n",
              "      <td>0.197682</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.231100</td>\n",
              "      <td>0.197300</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.210700</td>\n",
              "      <td>0.196999</td>\n",
              "      <td>0.951807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.208700</td>\n",
              "      <td>0.213132</td>\n",
              "      <td>0.963855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.193700</td>\n",
              "      <td>0.210044</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.200800</td>\n",
              "      <td>0.211119</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.197400</td>\n",
              "      <td>0.217884</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.205300</td>\n",
              "      <td>0.206685</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.206100</td>\n",
              "      <td>0.200096</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.183000</td>\n",
              "      <td>0.195373</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.163000</td>\n",
              "      <td>0.195691</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.146400</td>\n",
              "      <td>0.208384</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.164500</td>\n",
              "      <td>0.197540</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.145100</td>\n",
              "      <td>0.205092</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.138500</td>\n",
              "      <td>0.210994</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.157300</td>\n",
              "      <td>0.199369</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.144600</td>\n",
              "      <td>0.200692</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.137200</td>\n",
              "      <td>0.201349</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.141700</td>\n",
              "      <td>0.195705</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.153800</td>\n",
              "      <td>0.181377</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.146800</td>\n",
              "      <td>0.188348</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.188282</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.129900</td>\n",
              "      <td>0.209245</td>\n",
              "      <td>0.951807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.149500</td>\n",
              "      <td>0.190651</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.115400</td>\n",
              "      <td>0.184170</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.151600</td>\n",
              "      <td>0.186336</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.147700</td>\n",
              "      <td>0.188276</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.146500</td>\n",
              "      <td>0.197871</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.139200</td>\n",
              "      <td>0.186405</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.119700</td>\n",
              "      <td>0.179010</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.128100</td>\n",
              "      <td>0.189060</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.144600</td>\n",
              "      <td>0.194857</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.151900</td>\n",
              "      <td>0.189190</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.139100</td>\n",
              "      <td>0.186303</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.112400</td>\n",
              "      <td>0.193055</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.104500</td>\n",
              "      <td>0.189987</td>\n",
              "      <td>0.891566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.094100</td>\n",
              "      <td>0.172919</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.102200</td>\n",
              "      <td>0.166915</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.080400</td>\n",
              "      <td>0.175245</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.095700</td>\n",
              "      <td>0.189780</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.108800</td>\n",
              "      <td>0.177320</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.104500</td>\n",
              "      <td>0.185214</td>\n",
              "      <td>0.951807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.105600</td>\n",
              "      <td>0.193861</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.095200</td>\n",
              "      <td>0.179462</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.092600</td>\n",
              "      <td>0.192328</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.097200</td>\n",
              "      <td>0.176780</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.077600</td>\n",
              "      <td>0.183371</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.105200</td>\n",
              "      <td>0.177596</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.092900</td>\n",
              "      <td>0.184177</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.093900</td>\n",
              "      <td>0.176768</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.087800</td>\n",
              "      <td>0.177794</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.093300</td>\n",
              "      <td>0.187446</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.102400</td>\n",
              "      <td>0.182649</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.098000</td>\n",
              "      <td>0.182096</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.085500</td>\n",
              "      <td>0.177498</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>0.111500</td>\n",
              "      <td>0.178503</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.076400</td>\n",
              "      <td>0.177075</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>0.096400</td>\n",
              "      <td>0.185300</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.092400</td>\n",
              "      <td>0.184843</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>0.103900</td>\n",
              "      <td>0.184162</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.078900</td>\n",
              "      <td>0.185262</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>0.078700</td>\n",
              "      <td>0.183099</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.075600</td>\n",
              "      <td>0.178444</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.073900</td>\n",
              "      <td>0.182794</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>0.076100</td>\n",
              "      <td>0.178850</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>0.061100</td>\n",
              "      <td>0.178885</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>0.069100</td>\n",
              "      <td>0.178765</td>\n",
              "      <td>0.915663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>0.065800</td>\n",
              "      <td>0.177199</td>\n",
              "      <td>0.903614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.070400</td>\n",
              "      <td>0.181476</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>0.062100</td>\n",
              "      <td>0.182129</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>0.075900</td>\n",
              "      <td>0.180021</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>0.065500</td>\n",
              "      <td>0.179938</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.081700</td>\n",
              "      <td>0.179378</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.072200</td>\n",
              "      <td>0.178363</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>0.070500</td>\n",
              "      <td>0.178205</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>0.069300</td>\n",
              "      <td>0.180207</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.059900</td>\n",
              "      <td>0.181173</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>0.073200</td>\n",
              "      <td>0.180802</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.074600</td>\n",
              "      <td>0.180477</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>0.058200</td>\n",
              "      <td>0.180074</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>0.068600</td>\n",
              "      <td>0.179811</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>0.065600</td>\n",
              "      <td>0.179701</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>0.064800</td>\n",
              "      <td>0.179821</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.078500</td>\n",
              "      <td>0.180017</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>0.084400</td>\n",
              "      <td>0.179880</td>\n",
              "      <td>0.939759</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1265, training_loss=0.24892904635945798, metrics={'train_runtime': 5338.8911, 'train_samples_per_second': 1.894, 'train_steps_per_second': 0.237, 'total_flos': 3.66961176576e+18, 'train_loss': 0.24892904635945798, 'epoch': 5.0})"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "Kie1Gbvj_07Q",
      "metadata": {
        "id": "Kie1Gbvj_07Q"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "from peft import AutoPeftModel\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "oU31USsi_040",
      "metadata": {
        "id": "oU31USsi_040"
      },
      "outputs": [],
      "source": [
        "test_model = AutoPeftModel.from_pretrained('checkpoints\\\\checkpoint-1200')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "8189669f",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_model.to('cuda');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "0e6c9dfa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "    task='automatic-speech-recognition',\n",
        "    model=test_model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "14d3462c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'text': 'ഹലൂ എന്റെ പേര്യവാച്ചിത് ഞാന് കൊണ്ടോടിലിന്നുവരുന്നു പിന് അതുപല് തന്നെ ഞാന് ഒരു സംഭമമാണ് മാഹസംഭമാണ് അതുപലെ എല്ലാമാണ് കേൾക്കുന്നുണ്ട്ട്'}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe('record_out.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "681c6b59",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_input = common_voice['test'][100]['input_features']\n",
        "test_labels = common_voice['test'][100]['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a09b035",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_input = torch.tensor(test_input).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2dc41c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The following `model_kwargs` are not used by the model: ['langauge'] (note: typos in the generate arguments will also show up in this list)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m processor.decode(\u001b[43mtest_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtranscribe\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlangauge\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\peft\\peft_model.py:823\u001b[39m, in \u001b[36mPeftModel.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    822\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:774\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[39m\n\u001b[32m    765\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    767\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[32m    768\u001b[39m (\n\u001b[32m    769\u001b[39m     seek_sequences,\n\u001b[32m    770\u001b[39m     seek_outputs,\n\u001b[32m    771\u001b[39m     should_skip,\n\u001b[32m    772\u001b[39m     do_condition_on_prev_tokens,\n\u001b[32m    773\u001b[39m     model_output_type,\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:950\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n\u001b[32m    945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    946\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n\u001b[32m    947\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n\u001b[32m    948\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[32m    964\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2357\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2352\u001b[39m assistant_tokenizer = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33massistant_tokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[32m   2354\u001b[39m generation_config, model_kwargs = \u001b[38;5;28mself\u001b[39m._prepare_generation_config(\n\u001b[32m   2355\u001b[39m     generation_config, use_model_defaults, **kwargs\n\u001b[32m   2356\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2357\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[32m   2360\u001b[39m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1599\u001b[39m, in \u001b[36mGenerationMixin._validate_model_kwargs\u001b[39m\u001b[34m(self, model_kwargs)\u001b[39m\n\u001b[32m   1596\u001b[39m         unused_model_args.append(key)\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[32m-> \u001b[39m\u001b[32m1599\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1600\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (note: typos in the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1601\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m generate arguments will also show up in this list)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1602\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: The following `model_kwargs` are not used by the model: ['langauge'] (note: typos in the generate arguments will also show up in this list)"
          ]
        }
      ],
      "source": [
        "processor.decode(test_model.generate(input_features=test_input, task='transcribe', langauge='ml')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d87c8769",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\peft\\peft_model.py:823\u001b[39m, in \u001b[36mPeftModel.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    822\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:584\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[39m\n\u001b[32m    582\u001b[39m input_stride = \u001b[38;5;28mself\u001b[39m.model.encoder.conv1.stride[\u001b[32m0\u001b[39m] * \u001b[38;5;28mself\u001b[39m.model.encoder.conv2.stride[\u001b[32m0\u001b[39m]\n\u001b[32m    583\u001b[39m num_segment_frames = input_stride * \u001b[38;5;28mself\u001b[39m.config.max_source_positions\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m batch_size, total_input_frames = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retrieve_total_input_frames\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_stride\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_stride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    587\u001b[39m is_shortform = total_input_frames <= num_segment_frames\n\u001b[32m    589\u001b[39m \u001b[38;5;66;03m# 3. Make sure generation config is correctly set\u001b[39;00m\n\u001b[32m    590\u001b[39m \u001b[38;5;66;03m# Make sure the generation config is correctly set depending on whether timestamps are to be returned or not\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:1242\u001b[39m, in \u001b[36mWhisperGenerationMixin._retrieve_total_input_frames\u001b[39m\u001b[34m(input_features, input_stride, kwargs)\u001b[39m\n\u001b[32m   1239\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retrieve_total_input_frames\u001b[39m(input_features, input_stride, kwargs):\n\u001b[32m   1241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m input_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minput_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m[\u001b[32m0\u001b[39m], input_features.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m   1244\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m   1245\u001b[39m         encoder_outputs_shape = (\n\u001b[32m   1246\u001b[39m             kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m].shape\n\u001b[32m   1247\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], BaseModelOutput)\n\u001b[32m   1248\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m].shape\n\u001b[32m   1249\u001b[39m         )\n",
            "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "test_model.generate('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gDgOZepOjUhS",
      "metadata": {
        "id": "gDgOZepOjUhS"
      },
      "outputs": [],
      "source": [
        "def length_of_labels(batch):\n",
        "    return {\"label_len\" : len(batch['labels'])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A0W4iduw8S0l",
      "metadata": {
        "id": "A0W4iduw8S0l"
      },
      "outputs": [],
      "source": [
        "df = common_voice.map(length_of_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NnNK5g4-8lXB",
      "metadata": {
        "id": "NnNK5g4-8lXB"
      },
      "outputs": [],
      "source": [
        "train_label_length = df['train']['label_len']\n",
        "test_label_lenght = df['test']['label_len']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J74KTRcm9RVX",
      "metadata": {
        "id": "J74KTRcm9RVX"
      },
      "outputs": [],
      "source": [
        "processor.tokenizer.pad([{'input_ids':df['train'][0]['labels']}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pkuY34RJ87ru",
      "metadata": {
        "id": "pkuY34RJ87ru"
      },
      "outputs": [],
      "source": [
        "processor.tokenizer.pad()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
