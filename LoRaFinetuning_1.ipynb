{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "o28PTZGUfTyS",
      "metadata": {
        "id": "o28PTZGUfTyS"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade datasets -q\n",
        "# !pip install jiwer -q\n",
        "# !pip install evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "04cc562d",
      "metadata": {
        "id": "04cc562d"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torchaudio import load\n",
        "import evaluate\n",
        "\n",
        "# Load Whisper processor\n",
        "from transformers import WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer\n",
        "from transformers import WhisperForConditionalGeneration\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ffea934c",
      "metadata": {
        "id": "ffea934c"
      },
      "outputs": [],
      "source": [
        "\n",
        "load_dotenv()\n",
        "login_token = os.getenv('HuggingFaceToken')\n",
        "\n",
        "login(login_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce19b42",
      "metadata": {
        "id": "5ce19b42"
      },
      "source": [
        "## Data Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "31731923",
      "metadata": {
        "id": "31731923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
            "        num_rows: 2023\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
            "        num_rows: 710\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\n",
        "    \"mozilla-foundation/common_voice_17_0\", \"ml\", split=\"train+validation\",\n",
        ")\n",
        "common_voice[\"test\"] = load_dataset(\n",
        "    \"mozilla-foundation/common_voice_17_0\", \"ml\", split=\"test\",\n",
        ")\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "788cad4a",
      "metadata": {
        "id": "788cad4a"
      },
      "outputs": [],
      "source": [
        "# remove unwanted features\n",
        "common_voice = common_voice.select_columns(['audio', 'sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "71afaaba",
      "metadata": {
        "id": "71afaaba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'path': 'C:\\\\Users\\\\VICTUS\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\3e7b12b0fa0deddeccc4a37a644801109d30fe7dda8b39a953688d0be0744a2f\\\\ml_train_0/common_voice_ml_37003897.mp3', 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "       1.33694380e-06, 6.72575652e-07, 1.44025307e-07], shape=(150336,)), 'sampling_rate': 48000}\n"
          ]
        }
      ],
      "source": [
        "print(common_voice['train'][0]['audio'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6d5b848d",
      "metadata": {
        "id": "6d5b848d"
      },
      "outputs": [],
      "source": [
        "# 48kHz -> 16kHz\n",
        "from datasets import Audio\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f6c6b187",
      "metadata": {
        "id": "f6c6b187"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "       -1.61271237e-06, -1.26397367e-06,  1.32478658e-06], shape=(50112,))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "common_voice['train'][0]['audio']['array']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9f35b2e7",
      "metadata": {
        "id": "9f35b2e7"
      },
      "outputs": [],
      "source": [
        "# def collate_fuc(batch):\n",
        "#     print(len(batch))\n",
        "#     print(batch[0].keys())\n",
        "# data_loader = DataLoader(common_voice['train'], batch_size=3, shuffle=True, collate_fn=collate_fuc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1d45832d",
      "metadata": {
        "id": "1d45832d"
      },
      "outputs": [],
      "source": [
        "# for i in data_loader:\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b55252f5",
      "metadata": {
        "id": "b55252f5"
      },
      "outputs": [],
      "source": [
        "# filtering audio len > 30 sec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a75ce08e",
      "metadata": {
        "id": "a75ce08e"
      },
      "outputs": [],
      "source": [
        "# whisper proccessor wrap whisperFeature extractor for audio and whispertokenizer for text labels as one processor\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained('openai/whisper-small', task='transcribe', language='malayalam')\n",
        "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-small', task='transcribe', language='malayalam')\n",
        "processor = WhisperProcessor.from_pretrained('openai/whisper-small', task='transcribe', language='malayalam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6eea5ca5",
      "metadata": {
        "id": "6eea5ca5"
      },
      "outputs": [],
      "source": [
        "# sample_tokens = tokenizer.encode('ഇല്ല മോനേ')\n",
        "# tokenizer.decode(sample_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "db6558f9",
      "metadata": {
        "id": "db6558f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|startoftranscript|><|ml|><|transcribe|><|notimestamps|>ഇല്ല മോനേ<|endoftext|>'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'ഇല്ല മോനേ'\n",
        "batch = processor(text=text, sampling_rate=16000)\n",
        "processor.tokenizer.decode(batch['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7a621160",
      "metadata": {
        "id": "7a621160"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "def prepare_data(batch):\n",
        "    # processor have both feature extractor for audio and tokenizer for text, so we just pass both of theem\n",
        "    batch = processor(audio=batch['audio']['array'],\n",
        "                      text=batch['sentence'],\n",
        "                      sampling_rate=processor.feature_extractor.sampling_rate)\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "_smrP8WQclQ8",
      "metadata": {
        "id": "_smrP8WQclQ8"
      },
      "outputs": [],
      "source": [
        "MAX_LABEL_TOKEN_WHISPER_SUPPORT = 448\n",
        "def filter_label_token(batch):\n",
        "    return len(batch['labels']) <= MAX_LABEL_TOKEN_WHISPER_SUPPORT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6d9ce986",
      "metadata": {
        "id": "6d9ce986"
      },
      "outputs": [],
      "source": [
        "common_voice = common_voice.map(prepare_data, batched=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "GocjjBKwdAP1",
      "metadata": {
        "id": "GocjjBKwdAP1"
      },
      "outputs": [],
      "source": [
        "common_voice = common_voice.filter(filter_label_token, batched=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bb9dd23b",
      "metadata": {
        "id": "bb9dd23b"
      },
      "outputs": [],
      "source": [
        "common_voice = common_voice.select_columns(['input_features', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "bYnUncrDhDBR",
      "metadata": {
        "id": "bYnUncrDhDBR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_features', 'labels'],\n",
              "        num_rows: 2022\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_features', 'labels'],\n",
              "        num_rows: 710\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "common_voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7f734de1",
      "metadata": {
        "id": "7f734de1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 80, 3000])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.tensor(common_voice['train'][0]['input_features']).shape # (1, 80, 3000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291e40c5",
      "metadata": {
        "id": "291e40c5"
      },
      "source": [
        "Dataloader takes random datapoints, here it will look like {input_feature:.., labels}, when batch enabled it will be like [{inp:.., lable:..}, {inp: .., label:..}], we need to use data collator for pad them and join them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3f6badd5",
      "metadata": {
        "id": "3f6badd5"
      },
      "outputs": [],
      "source": [
        "# feature_extractor.pad(common_voice['train'][:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "94f20449",
      "metadata": {
        "id": "94f20449"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(common_voice['train'][0]['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b47615af",
      "metadata": {
        "id": "b47615af"
      },
      "outputs": [],
      "source": [
        "# Data Collator for padding\n",
        "\n",
        "class DataCollatorForSeqToSeqPadding:\n",
        "    def __init__(self, processor: WhisperProcessor):\n",
        "        self.processor = processor\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # batch = [ {'input_feature':[], labels:[]}, {} ...]\n",
        "        input_features = [{\"input_features\" : data['input_features'][0]} for data in batch]\n",
        "        labels = [{\"input_ids\" : data['labels']} for data in batch]\n",
        "\n",
        "        # feature extractor from hugging face already support padding to {'input_features':[]}\n",
        "        # padding using feature extractor for audio and tokenizer for labels\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors='pt') # created a batch object , later will add label to this too, that's how huggingface model expect data {'input_features':[], labels:[]}\n",
        "\n",
        "        # whisper tokenizer.pad will check the {'input_ids':[]} for padding and return in same forma\n",
        "        labels = self.processor.tokenizer.pad(labels, return_tensors='pt')\n",
        "\n",
        "\n",
        "        # since we are using hugging face model we don't need to stack the tensor cuz the hugging face (whisper here) model expect input like {'input_features':[], labels:[]}\n",
        "        # tensor stacking\n",
        "        # input_features = torch.stack(input_features, dim=0)\n",
        "        # labels = torch.stack(labels) # have\n",
        "\n",
        "\n",
        "        labels = labels['input_ids'].masked_fill(labels['attention_mask'].eq(0), -100)\n",
        "\n",
        "        # we are removing the start token since the hugging face model design to automatically add start token\n",
        "        # by doing shifting labels to right [1, 2, <\\s>] -> [<s>, 1, 2], where we using this shifted tensor as input\n",
        "        # and the non shifted as the labels to calculate the loss (the model gets what's his start token from the config)\n",
        "\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            print()\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "\n",
        "        return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "261d2ff8",
      "metadata": {
        "id": "261d2ff8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.bos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c849455e",
      "metadata": {
        "id": "c849455e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|endoftext|>'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(processor.tokenizer.bos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "380221a8",
      "metadata": {
        "id": "380221a8"
      },
      "outputs": [],
      "source": [
        "collate_fn = DataCollatorForSeqToSeqPadding(processor=processor)\n",
        "\n",
        "# data loader for just checking the data collator, seqtoseq trainer does not need dataloader (inbuilt)\n",
        "data_loader = DataLoader(dataset=common_voice['train'],\n",
        "                         collate_fn=collate_fn,\n",
        "                         batch_size=2,\n",
        "                         shuffle=True,\n",
        "                         drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "daa93162",
      "metadata": {
        "id": "daa93162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_features', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "for batch in data_loader:\n",
        "    print(batch.keys()) # torch.Size([2, 80, 3000]), Yes now it's coming as batch size and not in (2, 1, 80, 3000)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf94029b",
      "metadata": {
        "id": "bf94029b"
      },
      "source": [
        "## Model setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "bd32e5a6",
      "metadata": {
        "id": "bd32e5a6"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load('wer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5Fzcwve8jB7s",
      "metadata": {
        "id": "5Fzcwve8jB7s"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor.tokenizer.pad_token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "316f4a6b",
      "metadata": {
        "id": "316f4a6b"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    # pred will look like {'label_ids':[torch.tensor], prediction:[torch.tensor]}\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # change all -100 value which we set for loss calculation back to padding since we are calculating wer\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # convert to string and remove the padding token if there is.. if it was -100 then it won't work that is why we changed back to padding\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Now compute, the metric comes from the evaluate and we set it in the arugment of Trainer so compute metric use this metric here\n",
        "    wer = metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {'wer': wer} # standard form of hugging face"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f80821",
      "metadata": {
        "id": "94f80821"
      },
      "source": [
        "### Lora Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6aefe017",
      "metadata": {
        "id": "6aefe017"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType\n",
        "from peft import get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "798841f5",
      "metadata": {
        "id": "798841f5"
      },
      "outputs": [],
      "source": [
        "# Choose model size here\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "# forced decoder ids automatically add tokens at specified position (1, tokenizer.bos_token), so at decoder time the model automaticall generate it\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "460d1e91",
      "metadata": {
        "id": "460d1e91"
      },
      "outputs": [],
      "source": [
        "lora_r = 64\n",
        "lora_alpha = 64\n",
        "learning_rate = 5e-5\n",
        "peft_config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=0.01,\n",
        "    bias='none',\n",
        "    target_modules=['q_proj', 'v_proj', 'out_proj'],\n",
        "    task_type=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "44V2F7zpVpCW",
      "metadata": {
        "id": "44V2F7zpVpCW"
      },
      "outputs": [],
      "source": [
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "08CX2DRqepbc",
      "metadata": {
        "id": "08CX2DRqepbc"
      },
      "outputs": [],
      "source": [
        "model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "24771f94",
      "metadata": {
        "id": "24771f94"
      },
      "outputs": [],
      "source": [
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "680728b2",
      "metadata": {
        "id": "680728b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 10,616,832 || all params: 252,351,744 || trainable%: 4.2072\n"
          ]
        }
      ],
      "source": [
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8ec0c2af",
      "metadata": {
        "id": "8ec0c2af"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "1bd6dc1a",
      "metadata": {
        "id": "1bd6dc1a"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(output_dir='checkpoints',\n",
        "                                         eval_strategy='steps',\n",
        "                                         learning_rate=learning_rate,\n",
        "                                         gradient_checkpointing=True,\n",
        "                                         per_device_train_batch_size=4,\n",
        "                                         gradient_accumulation_steps=4,\n",
        "                                         warmup_steps=20,\n",
        "                                         predict_with_generate=True,\n",
        "                                         generation_max_length=30,\n",
        "                                         per_device_eval_batch_size=1,\n",
        "                                        #  eval_accumulation_steps=2,\n",
        "                                         fp16=True,\n",
        "                                         save_steps=100,\n",
        "                                         eval_steps=10,\n",
        "                                         logging_dir=f'runs/{lora_r}_{lora_alpha}_{learning_rate}',\n",
        "                                         report_to=['tensorboard'],\n",
        "                                         load_best_model_at_end=True,\n",
        "                                         metric_for_best_model='wer',\n",
        "                                         num_train_epochs=5,\n",
        "                                         torch_empty_cache_steps=5,\n",
        "                                         dataloader_drop_last=True,\n",
        "                                        #  dataloader_num_workers=2,\n",
        "                                        #  dataloader_pin_memory=True,\n",
        "                                         logging_strategy='steps',\n",
        "                                         logging_steps=10,\n",
        "                                         optim='adamw_torch',\n",
        "                                         label_names=['labels'],\n",
        "                                         lr_scheduler_type=\"cosine_with_restarts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "PITKfuOjquki",
      "metadata": {
        "id": "PITKfuOjquki"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric.compute(predictions=['hello', 'world'], references=['hello', 'there'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "76552312",
      "metadata": {
        "id": "76552312"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    train_dataset=common_voice['train'],\n",
        "    eval_dataset=common_voice['test'].shuffle(seed=0).select(range(20)),\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForSeqToSeqPadding(processor=processor),\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "2d16436d",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "0cd5ed5f",
      "metadata": {
        "id": "0cd5ed5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='634' max='635' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [634/635 29:04 < 00:05, 0.19 it/s, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.754500</td>\n",
              "      <td>0.824279</td>\n",
              "      <td>1.024096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.755900</td>\n",
              "      <td>0.809989</td>\n",
              "      <td>1.024096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.731100</td>\n",
              "      <td>0.797589</td>\n",
              "      <td>1.012048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.732700</td>\n",
              "      <td>0.787002</td>\n",
              "      <td>1.024096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.729700</td>\n",
              "      <td>0.771253</td>\n",
              "      <td>1.012048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.717600</td>\n",
              "      <td>0.766534</td>\n",
              "      <td>1.036145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.701100</td>\n",
              "      <td>0.761560</td>\n",
              "      <td>1.036145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.692500</td>\n",
              "      <td>0.745442</td>\n",
              "      <td>0.987952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.703600</td>\n",
              "      <td>0.735520</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.720058</td>\n",
              "      <td>1.012048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.674800</td>\n",
              "      <td>0.722358</td>\n",
              "      <td>1.012048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.688400</td>\n",
              "      <td>0.716232</td>\n",
              "      <td>1.012048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.645600</td>\n",
              "      <td>0.709027</td>\n",
              "      <td>0.987952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.636000</td>\n",
              "      <td>0.705008</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.630600</td>\n",
              "      <td>0.700286</td>\n",
              "      <td>0.951807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.636900</td>\n",
              "      <td>0.689003</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.665500</td>\n",
              "      <td>0.687063</td>\n",
              "      <td>0.963855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.646200</td>\n",
              "      <td>0.683953</td>\n",
              "      <td>0.963855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.627100</td>\n",
              "      <td>0.678794</td>\n",
              "      <td>0.987952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.626600</td>\n",
              "      <td>0.675486</td>\n",
              "      <td>0.987952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.576300</td>\n",
              "      <td>0.671575</td>\n",
              "      <td>0.963855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.632400</td>\n",
              "      <td>0.668588</td>\n",
              "      <td>0.963855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.621100</td>\n",
              "      <td>0.666048</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.605700</td>\n",
              "      <td>0.665115</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.593700</td>\n",
              "      <td>0.662622</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.600500</td>\n",
              "      <td>0.660466</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.621600</td>\n",
              "      <td>0.659675</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.609300</td>\n",
              "      <td>0.658592</td>\n",
              "      <td>0.963855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.602800</td>\n",
              "      <td>0.657994</td>\n",
              "      <td>0.963855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.604100</td>\n",
              "      <td>0.657288</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.606600</td>\n",
              "      <td>0.657287</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.602900</td>\n",
              "      <td>0.657136</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.631400</td>\n",
              "      <td>0.657144</td>\n",
              "      <td>0.975904</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=634, training_loss=0.3437549063835986, metrics={'train_runtime': 1748.3949, 'train_samples_per_second': 5.782, 'train_steps_per_second': 0.363, 'total_flos': 3.06912374784e+18, 'train_loss': 0.3437549063835986, 'epoch': 5.0})"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train(resume_from_checkpoint=\"checkpoints\\\\checkpoint-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "Kie1Gbvj_07Q",
      "metadata": {
        "id": "Kie1Gbvj_07Q"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "from peft import AutoPeftModel\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9f9ed14",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "oU31USsi_040",
      "metadata": {
        "id": "oU31USsi_040"
      },
      "outputs": [],
      "source": [
        "test_model = AutoPeftModel.from_pretrained('checkpoints\\\\checkpoint-600')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "8189669f",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_model.to('cuda');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "0e6c9dfa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "    task='automatic-speech-recognition',\n",
        "    model=test_model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "14d3462c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'text': 'ആലോ എന്നെ പര്രവാജ്ത്ത് ഞാന് കുണ്ട്ടിലുന്നുന്നും പുന്ന്നുപ്പ്പ്പ്പ്പ്പ്പുല്ല് എന്ന്നു സമുമമുനുനുനു ന്നുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനുനനുനുനുനുനുന�ുന�ു�'}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe('record_out.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "681c6b59",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_input = common_voice['test'][100]['input_features']\n",
        "test_labels = common_voice['test'][100]['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a09b035",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_input = torch.tensor(test_input).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2dc41c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The following `model_kwargs` are not used by the model: ['langauge'] (note: typos in the generate arguments will also show up in this list)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m processor.decode(\u001b[43mtest_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtranscribe\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlangauge\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\peft\\peft_model.py:823\u001b[39m, in \u001b[36mPeftModel.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    822\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:774\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[39m\n\u001b[32m    765\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    767\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[32m    768\u001b[39m (\n\u001b[32m    769\u001b[39m     seek_sequences,\n\u001b[32m    770\u001b[39m     seek_outputs,\n\u001b[32m    771\u001b[39m     should_skip,\n\u001b[32m    772\u001b[39m     do_condition_on_prev_tokens,\n\u001b[32m    773\u001b[39m     model_output_type,\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:950\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n\u001b[32m    945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    946\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n\u001b[32m    947\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n\u001b[32m    948\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[32m    964\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2357\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2352\u001b[39m assistant_tokenizer = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33massistant_tokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[32m   2354\u001b[39m generation_config, model_kwargs = \u001b[38;5;28mself\u001b[39m._prepare_generation_config(\n\u001b[32m   2355\u001b[39m     generation_config, use_model_defaults, **kwargs\n\u001b[32m   2356\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2357\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[32m   2360\u001b[39m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1599\u001b[39m, in \u001b[36mGenerationMixin._validate_model_kwargs\u001b[39m\u001b[34m(self, model_kwargs)\u001b[39m\n\u001b[32m   1596\u001b[39m         unused_model_args.append(key)\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[32m-> \u001b[39m\u001b[32m1599\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1600\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (note: typos in the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1601\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m generate arguments will also show up in this list)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1602\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: The following `model_kwargs` are not used by the model: ['langauge'] (note: typos in the generate arguments will also show up in this list)"
          ]
        }
      ],
      "source": [
        "processor.decode(test_model.generate(input_features=test_input, task='transcribe', langauge='ml')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d87c8769",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\peft\\peft_model.py:823\u001b[39m, in \u001b[36mPeftModel.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    822\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:584\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[39m\n\u001b[32m    582\u001b[39m input_stride = \u001b[38;5;28mself\u001b[39m.model.encoder.conv1.stride[\u001b[32m0\u001b[39m] * \u001b[38;5;28mself\u001b[39m.model.encoder.conv2.stride[\u001b[32m0\u001b[39m]\n\u001b[32m    583\u001b[39m num_segment_frames = input_stride * \u001b[38;5;28mself\u001b[39m.config.max_source_positions\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m batch_size, total_input_frames = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retrieve_total_input_frames\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_stride\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_stride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    587\u001b[39m is_shortform = total_input_frames <= num_segment_frames\n\u001b[32m    589\u001b[39m \u001b[38;5;66;03m# 3. Make sure generation config is correctly set\u001b[39;00m\n\u001b[32m    590\u001b[39m \u001b[38;5;66;03m# Make sure the generation config is correctly set depending on whether timestamps are to be returned or not\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\INTERNSHIP\\Whisper-Malayalam-Finetuning\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:1242\u001b[39m, in \u001b[36mWhisperGenerationMixin._retrieve_total_input_frames\u001b[39m\u001b[34m(input_features, input_stride, kwargs)\u001b[39m\n\u001b[32m   1239\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retrieve_total_input_frames\u001b[39m(input_features, input_stride, kwargs):\n\u001b[32m   1241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m input_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minput_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m[\u001b[32m0\u001b[39m], input_features.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m   1244\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m   1245\u001b[39m         encoder_outputs_shape = (\n\u001b[32m   1246\u001b[39m             kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m].shape\n\u001b[32m   1247\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], BaseModelOutput)\n\u001b[32m   1248\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m].shape\n\u001b[32m   1249\u001b[39m         )\n",
            "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "test_model.generate('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gDgOZepOjUhS",
      "metadata": {
        "id": "gDgOZepOjUhS"
      },
      "outputs": [],
      "source": [
        "def length_of_labels(batch):\n",
        "    return {\"label_len\" : len(batch['labels'])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A0W4iduw8S0l",
      "metadata": {
        "id": "A0W4iduw8S0l"
      },
      "outputs": [],
      "source": [
        "df = common_voice.map(length_of_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NnNK5g4-8lXB",
      "metadata": {
        "id": "NnNK5g4-8lXB"
      },
      "outputs": [],
      "source": [
        "train_label_length = df['train']['label_len']\n",
        "test_label_lenght = df['test']['label_len']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J74KTRcm9RVX",
      "metadata": {
        "id": "J74KTRcm9RVX"
      },
      "outputs": [],
      "source": [
        "processor.tokenizer.pad([{'input_ids':df['train'][0]['labels']}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pkuY34RJ87ru",
      "metadata": {
        "id": "pkuY34RJ87ru"
      },
      "outputs": [],
      "source": [
        "processor.tokenizer.pad()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
